%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Algorithms}
%\label{sec:algol}

%In the non-compliance setting there is additional information available to the algorithm. Ignoring the compliance-information (\chosen) reduces to the standard bandit setting. However, it should be possible to improve performance by taking advantage of observations about when treatments are \emph{actually} applied. Using compliance-information is not trivial, since bandit algorithms that rely purely on treatments (\actual) or purely on compliance (\comply) can have linear regret.

%This section proposes two hybrid algorithms that take advantage of compliance information, have bounded regret, and empirically outperform algorithms running the \chosen\, protocol.


\section{Worst Case Linear in T Regret in Non-Stationary setting to Compliance Unawareness}

Consider the regret not relative to a fixed best fixed action as usual, but relative to an algorithm that has access to compliance information. We show that this regret scales O(N) in the non-stationary setting 

if the regime from which rewards are drawn changes frequently enough, and within each regime the compliance awareness helps converge faster (for example due to very high rates of noncompliance of subjects that don't have different underlying characteristics, making \actual perform well, or due to subjects have information that helps them switch tot he best arm) 

\NDP{Formalize into a theorem, example/witness}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Hierarchical Algorithm}

A natural idea is to use the three protocols to train three experts and, simultaneously, learn which expert to apply. The resulting hierarchical bandit integrates compliance-information in a way that ensures the algorithm (i) has no-regret, because one of the base-algorithms uses \chosen, and therefore has no regret; and (ii) benefits from the compliance-information if it turns out to be useful.

The general construction is as follows. At the bottom-level are three bandit algorithms implementing the protocols \chosen, \actual\, and \comply. On the top-level is a fourth bandit algorithm whose arms are the three bottom-level algorithms. The top-level bandit learns which protocol is optimal. 
Note the top-level bandit is \emph{not} in an i.i.d. environment even when the external environment is i.i.d, since the low-level bandits are learning. We therefore use \texttt{EXP3} as the top-level bandit \cite{auer:02b}. 



\begin{algorithm}
   \caption{\texttt{HierarchicalBandit (HB)}}
   \label{alg:hier-exp}
   \begin{algorithmic}   
   \STATE {\bfseries Input:}
   	 Bandits $\cB_i$ running \texttt{NoRegretAlgorithm} on \chosen, \actual\, and \comply\, for $i =\{1,2,3\}$ respectively, with arms corresponding to treatments
   	 \STATE {\bfseries Input:}
   	 Bandit $\cH$ running \texttt{NoRegretAlgorithm}, with arms corresponding to $\cB_i$ above
    \FOR{$t=1$ {\bfseries to} $T$}
	\STATE Draw bandit $i\in\{1,2,3\}$ from $\cH$ and arm $j$ from $\cB_i$
	\STATE Pull arm $j$; incur loss $\ell^{(t)}$; observe compliance
	\STATE Update $\cH$ with loss applied to bandit-arm $i$
	\IF{$i=1$}
	\STATE Update $\cB_1$ with loss applied to treatment-arm $j$
	\ENDIF
	\STATE Update $\cB_{2/3}$ with loss according to relevant protocol
   	\ENDFOR

       	\end{algorithmic}
\end{algorithm}          



This section shows that constructing a hierarchical bandit with \texttt{EXP3} yields a no-regret algorithm. The result is straightforward; we include it for completeness. A similar result was shown in \cite{chang:05}. 
First, we construct a hierarchical version of \texttt{Hedge}, Algorithm~\ref{alg:meta-hedge}, which is applicable in the full-information setting. On the bottom-level are $M$ instantiations of \texttt{Hedge}. Instantiation $i$, for $i\in[M]$, plays an $N$-dimensional weight vector and receives $N$-dimensional loss vector $\loss^{(t)}_{i}$ on round $t$. We impose the assumption that all instantiations play $N$-vectors for notational convenience. The top-level is another instantiation of \texttt{Hedge}, which plays a weighted combination of the bottom-level instantiations.

\begin{algorithm}
   \caption{\texttt{Hierarchical Hedge (HHedge)}}
   \label{alg:meta-hedge}
   \begin{algorithmic}   
   	\STATE {\bfseries Input:} $\eta,\rho>0$\\
   	 $v^{(1)}_{i}=1$ for $i\in[M]$;\\ 
   	 $w^{(1)}_{i,j}=1$ for $(i,j)\in[M]\times[N]$
	\FOR{$t=1$ {\bfseries to} $T$}
	\STATE Set $\x^{(t)} \leftarrow \vt^{(t)}/X^{(t)}$ where $X^{(t)} = \sum_{i=1}^M v^{(t)}_{i}$.
	\STATE Set $\y^{(t)}_{i} \leftarrow \wt^{(t)}_{i}/Y^{(t)}_{i}$ where $Y^{(t)}_{i} = \sum_{j=1}^N w^{(t)}_{i,j}$.
	\STATE Receive feedback $\loss^{(t)}\in [0,1]^{M\times N}$ 
	\STATE Incur loss $\sum_{i,j=1}^{M,N} x^{(t)}_{i}\cdot\ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j}$
	\STATE Updates:
	\begin{align}
		v^{(t+1)}_i & \leftarrow v^{(t)}_{i}\cdot \exp\big(-\eta \sum_{j=1}^N\ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j}\big)
		\\
		w^{(t+1)}_{i,j} & \leftarrow w^{(t)}_{i,j}\cdot \exp\big(-\rho\cdot \ell^{(t)}_{i,j}\big)
	\end{align}
   	\ENDFOR
   	\end{algorithmic}
\end{algorithm}

We have the following lemma:

\begin{lem}\label{lem:meta-hedge}
	Introduce compound loss vector $\tilde{\loss}$ with $\tilde{\ell}^{(t)}_i := \sum_j \ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j}$. Then $\rho$ can be chosen in \texttt{HHedge} such that
	\begin{equation}
		\sum_{t=1}^T \langle \x^{(t)},\tilde{\loss}^{(t)}\rangle 
		\leq  \sum_{t=1}^T \tilde{\loss}^{(t)}_i +
		\leq O(\sqrt{T \log M})\quad\forall i.
	\end{equation}
	Moreover, $\rho$ and $\eta$ can be chosen such that, for all $i$,
	\begin{equation}
		\sum_{t,i,j=1}^{T,M,N} x^{(t)}_{i}\ell^{(t)}_{i,j} y^{(t)}_{i,j}
		\leq \sum_{t=1}^T\ell^{(t)}_{i,j}
		+ O(\sqrt{T \log M} + \sqrt{T \log N}).
	\end{equation}
\end{lem}

\begin{proof}
	Apply regret bounds for \texttt{Hedge} twice.
\end{proof}

Lemma~\ref{lem:meta-hedge} says, firstly, that \texttt{HHedge} has bounded regret relative to the bottom-level instantiations and, secondly, that it has bounded regret relative to any of the $M\times N$ experts on the bottom-level.


Algorithm~\ref{alg:meta-exp2} modifies \texttt{HHedge} so that it is suitable for bandit feedback, yielding \texttt{HEXP3}. A corresponding no-regret bound follows immediately:

\begin{lem}\label{lem:meta-exp}
	Define $\tilde{\loss}$ as in Lemma~\ref{lem:meta-hedge}. Then $\rho$ can be chosen in \texttt{HEXP3} such that
	\begin{equation}
		\expec\left[\sum_{t=1}^T\ell_{x^{(t)},y^{(t)}}\right]
		\leq \sum_{t=1}^T \tilde{\ell}^{(t)}_{i}
		+ O(\sqrt{MT\log M})
	\end{equation}
	Moreover, $\rho$ and $\eta$ can be chosen such that
	\begin{equation}
		\expec\left[\sum_{t=1}^{T} \ell^{(t)}_{x^{(t)},y^{(t)}}\right]
		\leq \sum_{t=1}^T\ell^{(t)}_{i,j}
		+ O(\sqrt{TM \log M} + \sqrt{T N\log N})
	\end{equation}
\end{lem}
\begin{proof}
	Follows from Lemma~\ref{lem:meta-hedge} and bounds for \texttt{EXP3}.
\end{proof}

\begin{algorithm}[tb]
   \caption{\texttt{Hierarchical EXP3 (HEXP3)}}
   \label{alg:meta-exp2}
   \begin{algorithmic}   
   \STATE {\bfseries Input:} $\eta,\rho>0$\\
   	 $v^{(1)}_{i}=1$ for $i\in[M]$;\\ 
   	 $w^{(1)}_{i,j}=1$ for $(i,j)\in[M]\times[N]$
	\FOR{$t=1$ {\bfseries to} $T$}
	\STATE Set $\x^{(t)} \leftarrow \vt^{(t)}/X^{(t)}$ where $X^{(t)} = \sum_{i=1}^M v^{(t)}_{i}$.
	\STATE Set $\y^{(t)}_{i} \leftarrow \wt^{(t)}_{i}/Y^{(t)}_{i}$ where $Y^{(t)}_{i} = \sum_{j=1}^N w^{(t)}_{i,j}$.
	\STATE Draw $x^{(t)}\sim \x^{(t)}$ and $y^{(t)}\sim \y^{(t)}_{x^{(t)}}$.
	\STATE Incur loss $\ell^{(t)}_{x^{(t)},y^{(t)}}\in [0,1]$ 
	\STATE Updates:
	\begin{align}
		v^{(t+1)}_i & \leftarrow \begin{cases}
			v^{(t)}_{i}\cdot 
			\exp\big(-\eta\frac{\ell^{(t)}_{i,j}}{x_i}\big) & i=x^{(t)} \\
			v^{(t)}_{i} & \text{else}
		\end{cases}		 
		\\
		w^{(t+1)}_{i,j} & \leftarrow \begin{cases}
			w^{(t)}_{i,j}\cdot \exp\big(-\rho\frac{\ell^{(t)}_{i,j}}{x_iy_{i,j}}\big) 
			& \text{if }(i,j)=(x^{(t)}, y^{(t)}) \\
			w^{(t)}_{i,j} &\text{else}
		\end{cases}
	\end{align}
   	\ENDFOR
   	\end{algorithmic}
\end{algorithm}


\begin{thm}[No-regret with respect to \actual, \comply\, and individual treatment advice]\label{thm:cexp}\eod
	Let \texttt{EXP3} be the no-regret algorithm used in Algorithm~\ref{alg:hier-exp} for both the bottom and top-level bandits, with suitable choice of learning rate. Then, \texttt{HB} satisfies
	\begin{equation}
		\expec\left[\sum_{t=1}^T\ell^{(t)}_{a^{(t)}}\right]
		\leq \sum_{t=1}^T \tilde{\ell}^{(t)}_{\actual/\comply}
		+ O(\sqrt{T})
	\end{equation}
	where $\tilde{\ell}^{(t)}_{\actual/\comply}$ denotes the expected loss vector of \texttt{EXP3} under the respective protocol on round $t$. 
	Furthermore, the regret against individual treatments $j\in[k]$ is bounded by
	\begin{equation}
		\expec\left[\sum_{t=1}^{T} \ell^{(t)}_{a^{(t)}}\right]
		\leq \sum_{t=1}^T\ell^{(t)}_{j}
		+ O(\sqrt{T k\log k})
	\end{equation}
\end{thm}

\begin{proof}
	Apply Lemma~\ref{lem:meta-exp} to $\texttt{HiearchicalBandit}$.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
