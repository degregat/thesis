%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model}
\label{sec:noncompliance}

This section introduces a formal setting for bandits with noncompliance and introduces protocols that prescribe how to make use of compliance information. Before diving into the formalism let us discuss, informally, how compliance information can be useful. 

First, suppose that the patient population is homogeneous in their response to the treatment, and that patients take the treatment with probability $p$ if prescribed and probability $1-p$ otherwise where $p<0.5$. In this setting, it is clear that a bandit algorithm will learn faster by rewarding arms according to whether the treatment was \emph{taken} by the patient, rather than whether it was \emph{recommended} to the patient. 

As a second example, consider \emph{corrective compliance} where patients who benefit from a treatment are more likely to take it, since they have access to information that the bandit does not. The bandit clearly benefits by learning from the information expressed in the behavior of the patients. Learning from the treatment actually taken is therefore more efficient than learning from the bandit's recommendations. Further examples are provided in section~\ref{sec:formal}.


%TODO: frame as a partial monitoring games, two armed equivalence ref %http://arxiv.org/pdf/1108.4961v1.pdf 
% Toward a classification of finite partial-monitoring games
% Partial-monitoring games constitute a mathematical framework for sequential decision making problems with imperfect feedback: the learner repeatedly chooses an action, the opponent responds with an outcome, and then the learner suffers a loss and receives a feedback signal, both of which are fixed functions of the action and the outcome. The goal of the learner is to minimize his total cumulative loss. We make progress toward the classification of these games based on their minimax expected regret. Namely, we classify almost all games with two outcomes and a finite number of actions: we show that their minimax expected regret is either zero, , Θ(T2/3), or Θ(T), and we give a simple and efficiently computable classification of these four classes of games. Our hope is that the result can serve as a stepping stone toward classifying all finite partial-monitoring games.


\begin{figure*}[t]
	\centering	
	\include{BCADAG}
	\caption{Bandit with Compliance Awareness DAG}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Formal setting}
\label{sec:formal}

We consider a sequential decision making problem where a process mediates between the actions chosen by the algorithm and the action carried out in the world. The general game is as follows:

\begin{defn}[bandit with compliance information]\label{def:compliance_bandit}\eod
	At each time-step $t$, the player selects an action $c_t\in \cA=[k]=\{1,\ldots,k\}$ (the chosen action). The environment responds by carrying out an action $a_t\in\cA=[k]$ (the actual action) and providing reward $r_t\in[0,1]$, or loss $\ell_t$.

	The standard bandit setting is when $a_t$ is either unobserved or $c_t = a_t$ for all $t\in[T]$.
\end{defn}
  
Given this model, we define the ``noncompliance level'' $n(c, u)$ for a specific choice $C=c$ and latent variables $U=u$ to mean the probability that $A \neq c$ given those values, that is, $n(c, u) = 1 - P(A=c|C=c, U=u)$.

Compliance and outcomes are often confounded. For example, healthy patients may be less inclined to take a treatment than unhealthy patients. 
The set of compliance-behaviors is the set of functions $\cC=\{\nu:\cA\rightarrow\cA\}$ from advice to treatment-taken \cite{koller:09}. 

\begin{defn}[model assumptions]\label{def:assumptions}\eod
	We make the following assumptions:
	\begin{enumerate}
		\item Compliance $\nu(u)\in\cC$ depends on a latent variable sampled i.i.d. from unknown  $\bP(u)$.
		\item Outcomes $r(\nu(u), a,u)$ depend on compliance-behavior, treatment-taken and the latent $u$. That is, outcomes are a fixed function $r:\cC\times \cA\times U\rightarrow[0,1]$.		
	\end{enumerate}
\end{defn}

The confounding of compliance with outcomes is modeled with a latent variable $u$ such that compliance $\nu(u)$ depends on $u$; and outcomes $r(c,\nu(u),u)$ is a function $r:\cA\times (\cA\rightarrow \cA)\times U\rightarrow [0,1]$ that depends on treatment-advice, compliance and the latent variable $u$.

When $|\cA|=k=2$ (control and treatment), we can list the compliance-behaviors explicitly.
\begin{defn}[compliance behaviors]\label{def:compliance_model}\eod
	For $k=2$, the following four subpopulations capture all deterministic compliance-behaviors:
	\begin{align}
		\text{never-takers ($\fN)$:}     \Big(c_0\mapsto a_0, c_1\mapsto a_0\Big)
		\qquad
		& \text{always-takers ($\fA)$:}    \Big(c_0\mapsto a_1, c_1\mapsto a_1\Big)
		\\
		\text{compliers ($\fC)$:} \Big( c_0\mapsto a_0, c_1\mapsto a_1\Big)
		\qquad
		&\text{defiers ($\fD)$:}    \Big(c_0\mapsto a_1 c_1\mapsto a_0\Big)
	\end{align}
	Let $p_s:= \expec_{u\sim \bP(u)}[\indic_{[\nu(u)=s]}]$ denote the probability of sampling from subpopulation $s\in\{\fN,\fA,\fC, \fD\}$.
\end{defn}
Unfortunately, the subpopulations cannot be distinguished from observations. For example, a patient that takes a prescribed treatment may be a complier or an always-taker. Nevertheless, observing compliance-behavior provides potentially useful side-information. The setting differs from contextual bandits because the side-information is only available \emph{after} the bandit chooses an arm. 	


\begin{defn}[stochastic reward model]\label{def:reward_model}\eod
	The expected reward given subpopulation $s$ and the actual treatment $j\in\cA$ is
	\begin{equation}
		r_{s,j} 
		:= \expec_{u\sim \bP(u)}\big[r(\nu(u),a_j,u)\,\big|\, \nu(u)=s\big]
		\quad\text{for }s\in \{\fN, \fA,\fC,\fD\}.
		\label{eq:exp_rew}		
	\end{equation}		
\end{defn}
The goal is to maximize the cumulative reward received,
i.e. choose a sequence of actions $(c_t)_{t\in T}$ that maximizes $\expec_{u\sim \bP(u)}\left[\sum_t r(\nu(u),\nu(u)(c_t), u)\right]$. We quantify the performance of algorithms in terms of regret, which compares the cumulative reward against that of the best action in hindsight.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reward protocols}
\label{sec:protocols}

Since compliance-information is only available after-pulling an arm, it cannot be used directly when selecting arms. However, how compliance-information can be used to modify the updates performed by the algorithm. For example, if the bandit recommends taking a treatment, and the patient does not do so, we have a choice about whether to update the arm that the bandit recommended (treatment) or the arm that the patient pulled (control).
Each protocol can be combined with any multi-armed bandit algorithm. 

\begin{defn}[reward protocols]\label{def:protocols}\eod
	We consider three protocols for assigning rewards to arms:
	\begin{enumerate}[P1.]
		\item \textbf{\chosen: chosen-treatment updates.}
		Assign reward $r_t$ to arm $j$ if $c_t=j$.
		\item \textbf{\actual: actual-treatment updates.}
		Assign reward $r_t$ to arm $j$ if $a_t=j$.
		\item \textbf{\comply: compliance-based updates.}
		Assign reward $r_t$ to arm $j$ if $c_t=j$ and $a_t=j$.
	\end{enumerate}
\end{defn}
The protocols rewards are summarized in the table bellow. Each protocol has strengths and weaknesses.

\subsection{Rewards assigned to arms by the three protocols}
\label{sec:protocol_table}

The rewards assigned to each arm by the three protocols are summarized in the table below. None of the protocols successfully isolates the compliers. It follows, as seen above, that which protocol is optimal depends on the structure of the population, which is unknown to the learner. The table can be extended with additional reward protocols. Here we restrict attention to the three most intuitive protocols.


\begin{center}
\begin{tabular}{| l | c | c | c |}
\hline
Arm updated & \chosen & \actual & \comply \\
\hline
      & $r_{\fN,0}$ & $r_{\fN,0}$ & $r_{\fN,0}$ \\
$i=0$ & $r_{\fC,0}$ & $r_{\fC,0}$ & $r_{\fC,0}$ \\
      & $r_{\fA,1}$ &             &             \\
      & $r_{\fD,1}$ & $r_{\fD,0}$ &             \\
\hline
      & $r_{\fN,0}$ &             &             \\
$i=1$ & $r_{\fC,1}$ & $r_{\fC,1}$ & $r_{\fC,1}$ \\
      & $r_{\fA,1}$ & $r_{\fA,1}$ & $r_{\fA,1}$ \\
      & $r_{\fD,0}$ & $r_{\fD,1}$ &             \\
\hline
\end{tabular}
\end{center}





\paragraph{Protocol \#1: \chosen.}
Under \chosen, the bandit advises the patient on which treatment to take, and ignores whether or not the patient complies. 

\begin{prop}\label{prop:chosen}
	Standard regret bounds hold for any algorithm under \chosen.
\end{prop}


\paragraph{Protocol \#2: \actual.} 
Expected rewards depend on the treatment Eq.~\eqref{eq:exp_rew} chosen by the patient, and not directly on the arm pulled by the bandit. Thus, a natural alternative to \chosen\, is \actual, where the bandit assigns rewards to the treatment that the patient actually used -- which may not in general coincide with the arm that the bandit pulled.

\begin{prop}\label{prop:actual}
	There are settings where \actual\, outperforms \chosen\, and \comply.
\end{prop}
\begin{proof}
	Suppose that $r_{s,j}=r_j$ depends on the treatment but not the subpopulation. Further suppose the population is a mix of always-takers, never-takers, and compliers -- but no defiers. 
	Always-takers and never-takers ignore the bandit, which therefore only interacts with the compliers. 

	The rewards used to update \chosen\, are, in expectation 
	\begin{equation}
		\expec[R\,|\,c_0] 
		= (1-p_\fA)\cdot r_0 + p_\fA\cdot r_1
	\end{equation} and 
	\begin{equation}
		\expec[R\,|\,c_1]
		= p_\fN\cdot r_0 + (1-p_\fN)\cdot r_1
	\end{equation}
	whereas the rewards used to update \actual\, are $\expec[R\,|\,a_0] = r_0$ and $\expec[R\,|\,a_1] = r_1$.
	It follows that
	\begin{equation}
		r_{\fC,0} = \expec[R\,|\,a_0] \neq \expec[R\,|\,c_0] 
	\end{equation} and 
	\begin{equation}
		r_{\fC,1} = \expec[R\,|\,a_1] \neq \expec[R\,|\,c_1].
	\end{equation}
	Thus, \actual\, assigns rewards to arms based on their effect on compliers (which are the only subpopulation interacting with the bandit), whereas the rewards assigned to arms by \chosen\, are diluted by patients who do not take the treatment. Finally, \actual\, outperforms \comply\, because it updates more frequently.
\end{proof}

However, \actual\, can fail completely.
\begin{eg}[\actual\, has linear regret; defiers]\label{eg:defiers}\eod
	Suppose that the population consists in defiers and further suppose the treatment has a positive effect: $r_{\fD,0}=0$ and $r_{\fD,1}=1$.
	Bandit algorithms using protocol \#2 will learn to pull arm $c_1$, causing defiers to pull arm $0$. The best move in hindsight is the opposite.
\end{eg}
Defiers are arguably a pathological case. The next scenario is more realistic in clinical trials:
\begin{eg}[Linear regret; harmful treatment]\label{eg:rich}\eod
	Suppose there are two sub-populations: the first consists of rich, healthy patients who always take the treatment. The second consists of poor, less healthy patients who only take the treatment if prescribed. Finally, suppose the treatment \emph{reduces} well-being by $0.25$ on some metric. We then have
	\begin{equation}
	    \expec[R|a_0] = p_\fC \cdot r_{\fC,0} = 0p_\fC 
	\end{equation} and 
	
	\begin{equation}
	    \expec[R|a_1] = p_\fC \cdot r_{\fC,1} + p_\fA\cdot r_{\fA,1}
	    = -0.25p_\fC + 0.75p_\fA
	\end{equation}
	If the population of healthy always-takers $p_\fA$ is sufficiently large, then \actual\, assigns higher rewards to the \emph{harmful} treatment arm.
\end{eg}


\paragraph{Protocol \#3: \comply.}
Finally, \chosen\, and \actual\, can be combined to form \comply, which only rewards an arm if (i) it was chosen by the bandit and (ii) the patient followed the bandit's advice.

\begin{prop}\label{prop:comply}
	There are settings where \comply\, outperforms \chosen\, and \actual.
\end{prop}

\begin{proof}
	It is easy to see that \comply\, outperforms \chosen\, in the setting of Proposition~\ref{prop:actual}.
	
	Consider a population of never-takers, always-takers and compliers. Suppose that never-takers are healthier than compliers $r_{\fN,0}> r_{\fC,0/1}$ whereas always-takers are less healthy $r_{\fA,1}<r_{\fC,0/1}$.  
    The expected rewards received by \chosen\, are
	\begin{align}
		\expec[R\,|\,c_0]
		& = p_{\fC}r_{\fC,0} + p_\fN r_{\fN,0} + p_\fA r_{\fA,1}
		\\
		\expec[R\,|\,c_1]
		& = p_{\fC}r_{\fC,1} + p_\fN r_{\fN,0} + p_\fA r_{\fA,1}
	\end{align}
	
	Let $q_0$ and $q_1$ be the probability that the bandit pulls arms 0 and 1 respectively. The expected rewards received by \actual\, are
	\begin{equation}
		\expec[R\,|\,a_0]
		 = \frac{q_0 p_{\fC}r_{\fC,0} + p_\fN r_{\fN,0}}{q_0p_\fC + p_\fN}		
		\expec[R\,|\,a_1]
		\quad\text{and}\quad
		 = \frac{p_\fA r_{\fA,1} + q_1p_\fC r_{\fC,1}}{p_\fA + q_1 p_\fC}
	\end{equation}
	whereas the rewards used to update \comply\, are
	\begin{align}
		\expec[R\,|\,c_0,a_0] = \frac{p_{\fC}r_{\fC,0} + p_\fN r_{\fN,0}}{p_\fC + p_\fN}
		\quad\text{and}\quad
		\expec[R\,|\,c_1,a_1] = \frac{p_\fA r_{\fA,1} + p_{\fC}r_{\fC,1}}{p_\fA + p_\fC}
	\end{align}
	It follows that $r_{\fC,0} \leq\expec[R\,|\,c_0,a_0]\leq \expec[R\,|\,a_0]$ and $r_{\fC,} \geq\expec[R\,|\,c_1,a_1] \geq \expec[R\,|\,a_1]$.
	The reward estimates for compliers are diluted under both \texttt{Actual} and \texttt{Comply}. However, \comply's estimate is more accurate.
\end{proof}

It is easy to see that \comply\, also has unbounded regret on example~\ref{eg:rich}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
